{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pravallika12345678/FMML_Assignments/blob/main/Module_01_Lab_03_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3yfry25JgZK"
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "FMML Module 1, Lab 3<br>\n",
        "\n",
        " In this lab, we will see how augmentation of data samples help in improving the machine learning performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZU8_elooqP0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)\n",
        "from sklearn.utils.extmath import cartesian\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5pHYogSMHiE"
      },
      "source": [
        "Augmentation is useful when we have less training data available. Augmentation allows us to 'create' a larger dataset programatically.\n",
        "\n",
        "For this lab we will use a subset of MNIST that is very small, to better understand the effect of augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJvmWJ58ovx5"
      },
      "source": [
        "#loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "train_X = train_X[::1200,:,:].copy() # subsample. Otherwise it will take too long!\n",
        "train_y = train_y[::1200].copy() # do the same to the labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XamH6z1Rt7S"
      },
      "source": [
        "Let us borrow a few functions from the previous labs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk2W5_3BRLMS"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  traindata = traindata.reshape(-1, 28*28)\n",
        "  testdata = testdata.reshape(-1,28*28)\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGiA3LmDSJZo"
      },
      "source": [
        "In this lab, we will use the image pixels themselves as features, instead of extracting features. Each image has 28*28 pixels, so we will flatten them to 784 pixels to use as features. Note that this is very compute intensive and will take a long time.<br>\n",
        "\n",
        "Let us check the baseline accuracy on the test set without any augmentations. We hope that adding augmentations will help us to get better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tQvnoasRNEV"
      },
      "source": [
        "testpred = NN(train_X, train_y, test_X)\n",
        "print('Baseline accuracy without augmentation is ', Accuracy(test_y, testpred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfkcMfhIZQ7U"
      },
      "source": [
        "Let us try to improve this accuracy using augmentations. When we create augmentations, we have to make sure that the changes reflect what will naturally occur in the dataset. For example, we should not add colour to our samples as an augmentation because they do not naturally occur. We should not also flip the images in MNIST, because flipped images have different meanings for digits.\n",
        "\n",
        "### Augmentation 1: rotation\n",
        "\n",
        "Let us try rotating the image a little. We will use skimage library for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5WolJ9fZE7L"
      },
      "source": [
        "plt.imshow(train_X[2], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(rotate(train_X[2],25), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE33Yxgggu0c"
      },
      "source": [
        "After rotating, the the class of the image is still the same. Let us make a function to rotate multiple images by random angles. We want a slightly different image every time we run this function. So, we generate a random number between 0 and 1 and change it so that it lies between -constraint/2 and +constraint/2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyM7pUV7Reze"
      },
      "source": [
        "def augRotate(sample, angleconstraint):\n",
        "  if angleconstraint==0:\n",
        "    return sample\n",
        "  if len(sample.shape)==2:\n",
        "    sample = np.expand_dims(sample, 0)  # make sure the sample is 3 dimensional\n",
        "  angle = rng.random(len(sample)) # generate random numbers for angles\n",
        "  angle = (angle-0.5)*angleconstraint # make the random angle constrained\n",
        "  nsample = sample.copy() # preallocate the augmented array to make it faster\n",
        "  for ii in range(len(sample)):\n",
        "    nsample[ii] = rotate(sample[ii], angle[ii])\n",
        "  return np.squeeze(nsample) # take care if the input had only one sample."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDk-N5VNjar9"
      },
      "source": [
        "This function returns a slightly different image each time we call it. So we can increase the number of images in the sample by any multiple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw3O9zGFgI8K"
      },
      "source": [
        "sample = train_X[20]\n",
        "angleconstraint = 70\n",
        "# show the original image\n",
        "plt.imshow(sample, cmap='gray')\n",
        "plt.show()\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(augRotate(sample, angleconstraint), cmap='gray') # show an augmented image\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(augRotate(sample, angleconstraint), cmap='gray') # show another augmented image from the same sample\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(augRotate(sample, angleconstraint), cmap='gray') # one more image from the same sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytv3NxF-kgxN"
      },
      "source": [
        "Let us augment the whole dataset and see if this improves the test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNzNAoDBkRzj"
      },
      "source": [
        "# hyperparameters\n",
        "angleconstraint = 60\n",
        "naugmentations = 5\n",
        "\n",
        "# augment\n",
        "augdata = train_X # we include the original images also in the augmented dataset\n",
        "auglabel = train_y\n",
        "for ii in range(naugmentations):\n",
        "  augdata = np.concatenate((augdata, augRotate(train_X, angleconstraint))) # concatenate the augmented data to the set\n",
        "  auglabel = np.concatenate((auglabel, train_y))  # the labels don't change when we augment\n",
        "\n",
        "# check the test accuracy\n",
        "testpred = NN(augdata, auglabel, test_X)\n",
        "print('Accuracy after rotation augmentation is ', Accuracy(test_y, testpred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E88Nt9s1p5R6"
      },
      "source": [
        "The angle constraint is a hyperparameter which we have to tune using a validation set. (Here we are not doing that for time constraints). Let us try a grid search to find the best angle constraint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiaFRLREmGp6"
      },
      "source": [
        "angleconstraints = [0,10,20,30,40,50,60,70,80,90] # the values we want to test\n",
        "accuracies = np.zeros(len(angleconstraints), dtype=np.float) # we will save the values here\n",
        "\n",
        "for ii in range(len(angleconstraints)):\n",
        "  # create the augmented dataset\n",
        "  augdata = train_X # we include the original images also in the augmented dataset\n",
        "  auglabel = train_y\n",
        "  for jj in range(naugmentations):\n",
        "    augdata = np.concatenate((augdata, augRotate(train_X, angleconstraints[ii]))) # concatenate the augmented data to the set\n",
        "    auglabel = np.concatenate((auglabel, train_y))  # the labels don't change when we augment\n",
        "\n",
        "  # check the test accuracy\n",
        "  testpred = NN(augdata, auglabel, test_X)\n",
        "  accuracies[ii] = Accuracy(test_y, testpred)\n",
        "  print('Accuracy after rotation augmentation constrained by ',angleconstraints[ii], ' is ', accuracies[ii], flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oVDRYP2rxob"
      },
      "source": [
        "Let us see the best value for angle constraint: (Ideally this should be done on validation set, not test set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqthJa_pmMHz"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
        "# plot the variation of accuracy\n",
        "ax.plot(angleconstraints, accuracies)\n",
        "ax.set_xlabel('angle')\n",
        "ax.set_ylabel('accuracy')\n",
        "# plot the maximum accuracy\n",
        "maxind = np.argmax(accuracies)\n",
        "plt.scatter(angleconstraints[maxind], accuracies[maxind], c='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ8YuVfCuGTj"
      },
      "source": [
        "Let us try one more augmentation: shear. Here is what this looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMiw46NLwssK"
      },
      "source": [
        "def shear(sample, amount):\n",
        "  tform = AffineTransform(shear = amount) # create the shear transform\n",
        "  img = warp(sample, tform) # apply the shear\n",
        "  # this makes the digit off-center. Since all the images in the test set are centralized, we will do the same here\n",
        "  col = img.sum(0).nonzero()[0]\n",
        "  row = img.sum(1).nonzero()[0]\n",
        "  if len(col)>0 and len(row)>0:\n",
        "    xshift = int(sample.shape[0]/2 - (row[0]+row[-1])/2)\n",
        "    yshift = int(sample.shape[1]/2 - (col[0]+col[-1])/2)\n",
        "    img = np.roll(img, (xshift, yshift),(0,1))\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_u_EYpmnABK"
      },
      "source": [
        "sample = train_X[2]\n",
        "plt.imshow(sample, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# apply shear\n",
        "plt.imshow(shear(sample, 0.4), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnWMoyM2pK4"
      },
      "source": [
        "Create an augmentation function which applies a random shear according to the constraint we provide:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qLDJyGytwP5"
      },
      "source": [
        "def augShear(sample, shearconstraint):\n",
        "  if shearconstraint==0:\n",
        "    return sample\n",
        "  if len(sample.shape)==2:\n",
        "    sample = np.expand_dims(sample, 0)  # make sure the sample is 3 dimensional\n",
        "  amt = rng.random(len(sample)) # generate random numbers for shear\n",
        "  amt = (amt-0.5)*shearconstraint # make the random shear constrained\n",
        "  nsample = sample.copy() # preallocate the augmented array to make it faster\n",
        "  for ii in range(len(sample)):\n",
        "    nsample[ii] = shear(sample[ii], amt[ii])\n",
        "  return np.squeeze(nsample) # take care if the input had only one sample."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6lQcWW93suJ"
      },
      "source": [
        "Let us do a grid search to find the best shear constraint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_wrqPkrzBb_"
      },
      "source": [
        "shearconstraints = [0, 0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0] # the values we want to test\n",
        "accuracies = np.zeros(len(shearconstraints), dtype=np.float) # we will save the values here\n",
        "\n",
        "for ii in range(len(shearconstraints)):\n",
        "  # create the augmented dataset\n",
        "  augdata = train_X # we include the original images also in the augmented dataset\n",
        "  auglabel = train_y\n",
        "  for jj in range(naugmentations):\n",
        "    augdata = np.concatenate((augdata, augShear(train_X, shearconstraints[ii]))) # concatenate the augmented data to the set\n",
        "    auglabel = np.concatenate((auglabel, train_y))  # the labels don't change when we augment\n",
        "\n",
        "  # check the test accuracy\n",
        "  testpred = NN(augdata, auglabel, test_X)\n",
        "  accuracies[ii] = Accuracy(test_y, testpred)\n",
        "  print('Accuracy after shear augmentation constrained by ',shearconstraints[ii], ' is ', accuracies[ii], flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKaH-YR-zVnA"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
        "# plot the variation of accuracy\n",
        "ax.plot(shearconstraints, accuracies)\n",
        "ax.set_xlabel('angle')\n",
        "ax.set_ylabel('accuracy')\n",
        "# plot the maximum accuracy\n",
        "maxind = np.argmax(accuracies)\n",
        "plt.scatter(shearconstraints[maxind], accuracies[maxind], c='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccfdbRcQ7Zgg"
      },
      "source": [
        "We can do multiple augmentations at the same time. Here is a function to do both shear and rotation to the sample. In this case, we will have two hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh8S_Pxa0XCv"
      },
      "source": [
        "def augRotateShear(sample, angleconstraint, shearconstraint):\n",
        "  if len(sample.shape)==2:\n",
        "    sample = np.expand_dims(sample, 0)  # make sure the sample is 3 dimensional\n",
        "  amt = rng.random(len(sample)) # generate random numbers for shear\n",
        "  amt = (amt-0.5)*shearconstraint # make the random shear constrained\n",
        "  angle = rng.random(len(sample)) # generate random numbers for angles\n",
        "  angle = (angle-0.5)*angleconstraint # make the random angle constrained\n",
        "  nsample = sample.copy() # preallocate the augmented array to make it faster\n",
        "  for ii in range(len(sample)):\n",
        "    nsample[ii] = rotate(shear(sample[ii], amt[ii]), angle[ii]) # first apply shear, then rotate\n",
        "  return np.squeeze(nsample) # take care if the input had only one sample."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGKyjjNx-NQ4"
      },
      "source": [
        "Since we have two hyperparameters, we have to do the grid search on a 2 dimensional matrix. We can use our previous experience to inform where to search for the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJC45WRg0pOP"
      },
      "source": [
        "shearconstraints = [0, 0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6] # the values we want to test\n",
        "angleconstraints = [0,10,20,30,40,50,60] # the values we want to test\n",
        "hyp = cartesian((shearconstraints, angleconstraints)) # cartesian product of both\n",
        "\n",
        "accuracies = np.zeros(len(hyp), dtype=np.float) # we will save the values here\n",
        "\n",
        "for ii in range(len(hyp)):\n",
        "  # create the augmented dataset\n",
        "  augdata = train_X # we include the original images also in the augmented dataset\n",
        "  auglabel = train_y\n",
        "  for jj in range(naugmentations):\n",
        "    augdata = np.concatenate((augdata, augRotateShear(train_X, hyp[ii][0], hyp[ii][1]))) # concatenate the augmented data to the set\n",
        "    auglabel = np.concatenate((auglabel, train_y))  # the labels don't change when we augment\n",
        "\n",
        "  # check the test accuracy\n",
        "  testpred = NN(augdata, auglabel, test_X)\n",
        "  accuracies[ii] = Accuracy(test_y, testpred)\n",
        "  print('Accuracy after augmentation shear:',hyp[ii][0], 'angle:',hyp[ii][1], ' is ', accuracies[ii], flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT6CnvSDEX7a"
      },
      "source": [
        "Let us plot it two dimensionally to see which is the best value for the hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD2i7msI_cLd"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
        "im = ax.imshow(accuracies.reshape((len(shearconstraints), len(angleconstraints))), cmap='inferno')\n",
        "ax.set_xlabel('angle')\n",
        "ax.set_ylabel('shear')\n",
        "ax.set_xticks(np.arange(len(angleconstraints)));\n",
        "ax.set_xticklabels(angleconstraints);\n",
        "ax.set_yticks(np.arange(len(shearconstraints)));\n",
        "ax.set_yticklabels(shearconstraints);\n",
        "plt.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcZWJiFJDMh"
      },
      "source": [
        "It seems that rotation and shear don't mix! The best accuracy is when rotation is zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAasQo1C3x4A"
      },
      "source": [
        "## Questions\n",
        "Try these questions for better understanding. You may not be able to solve all of them.\n",
        "1. What is the best value for angle constraint and shear constraint you got? How much did the accuracy improve as compared to not using augmentations?\n",
        "2. Can you increase the accuracy by increasing the number of augmentations from each sample?\n",
        "3. Try implementing a few augmentations of your own and experimenting with them. A good reference is <a href=https://www.analyticsvidhya.com/blog/2019/12/image-augmentation-deep-learning-pytorch/>here. </a>\n",
        "4. Try combining various augmentations. What is the highest accuracy you can get? What is the smallest training dataset you can take and still get accuracy above 50%?\n",
        "\n",
        "Whenever you do any experiment, a good practice is to vary the hyperparameters gradually and create a graph of your results, like we did for gridsearch."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.ans"
      ],
      "metadata": {
        "id": "olfZDJKj6e1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I don't have access to specific, real-time data or experiments, and my knowledge is based on information available up to September 2021. The optimal values for angle constraints and shear constraints in data augmentation can vary depending on the specific task and dataset you are working with. These values are typically chosen through experimentation and can vary for different machine learning projects.\n",
        "\n",
        "To determine the best values for angle and shear constraints for your particular project, you should conduct experiments with different values and evaluate their impact on the performance of your machine learning model. The improvement in accuracy by using augmentations also depends on several factors, including the quality of your dataset, the complexity of the task, and the architecture of your model.\n",
        "\n",
        "In general, data augmentation techniques are used to increase the diversity of the training data, which can help improve the generalization and robustness of your model. The extent to which accuracy improves with data augmentation will vary from one project to another.\n",
        "\n",
        "To find the best values for angle and shear constraints and assess the improvement in accuracy, you can follow these steps:\n",
        "\n",
        "Define a range of values for angle and shear constraints that you want to experiment with.\n",
        "\n",
        "Apply these augmentations to your training data and train your model with different augmentation settings.\n",
        "\n",
        "Evaluate the model's performance on a validation set or through cross-validation for each set of augmentation parameters.\n",
        "\n",
        "Compare the performance metrics (such as accuracy, precision, recall, F1-score, etc.) of models with different augmentation settings.\n",
        "\n",
        "Select the augmentation parameters that result in the best performance on your validation set or cross-validation.\n",
        "\n",
        "Compare the performance of the model with the selected augmentation parameters to the model trained without data augmentation to determine the improvement in accuracy.\n",
        "\n",
        "Keep in mind that the optimal values for augmentation parameters may not necessarily be the same for all datasets and tasks, so it's important to experiment and fine-tune these parameters based on your specific project's requirements."
      ],
      "metadata": {
        "id": "1wCiVoLbNb4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.ans"
      ],
      "metadata": {
        "id": "e2vfihjTNeGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Increasing the number of augmentations from each sample can potentially help improve the accuracy of your machine learning model, but there are some important considerations to keep in mind:\n",
        "\n",
        "Data Diversity: Augmentations are primarily used to increase the diversity of your training data. By applying multiple augmentations to each sample, you expose your model to a wider range of variations within the data, which can help it generalize better to unseen examples.\n",
        "\n",
        "Overfitting: While data augmentation can be a powerful technique, applying too many augmentations to each sample can risk overfitting. Overfitting occurs when a model becomes too specialized in capturing the variations in the training data but fails to generalize well to new, unseen data. If you excessively augment your data, the model may memorize these augmented examples rather than learning the underlying patterns.\n",
        "\n",
        "Computational Resources: Increasing the number of augmentations for each sample also increases the computational resources required for training. This can result in longer training times and potentially higher memory requirements.\n",
        "\n",
        "Balancing Act: It's essential to strike a balance between applying enough augmentations to enhance data diversity and avoiding overfitting. The optimal number of augmentations may vary depending on your dataset and task.\n",
        "\n",
        "To determine the impact of increasing the number of augmentations, you can perform the following steps:\n",
        "\n",
        "Start with a reasonable number of augmentations per sample, such as 2 or 3.\n",
        "\n",
        "Train your model and evaluate its performance on a validation set. Monitor metrics like accuracy, loss, and validation error.\n",
        "\n",
        "Gradually increase the number of augmentations per sample and retrain your model at each step. Continue to monitor performance on the validation set. Observe how the model's performance changes with increasing augmentations. At some point, you may notice diminishing returns or even a drop in performance if you over-augment.\n",
        "\n",
        "Select the number of augmentations that result in the best performance on your validation set. This is the point where the model generalizes well without overfitting.\n",
        "\n",
        "Remember that the optimal number of augmentations can vary depending on your dataset's size, complexity, and the specific task you're addressing. Experimentation is crucial to finding the right balance and determining the most suitable number of augmentations for your particular use case.\n",
        "\n"
      ],
      "metadata": {
        "id": "0g5ek8VvNgLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.ans"
      ],
      "metadata": {
        "id": "psXLJsArNu1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I don't have the capability to implement and experiment with augmentations directly, as I'm a text-based AI model and do not have access to image processing libraries or the ability to perform real-time computations. However, I can provide you with some simple examples of image augmentations and guide you on how to implement and experiment with them using popular image processing libraries like Python's OpenCV or libraries designed for deep learning tasks like TensorFlow or PyTorch.\n",
        "\n",
        "Here are a few basic image augmentations you can experiment with:\n",
        "\n",
        "Rotation: Rotate the image by a random angle within a specified range to increase robustness to orientation variations.\n",
        "\n",
        "Flip: Horizontally flip the image randomly to account for horizontal symmetry.\n",
        "\n",
        "Brightness and Contrast Adjustment: Randomly adjust the brightness and contrast of the image to simulate varying lighting conditions.\n",
        "\n",
        "Zoom and Crop: Randomly zoom into or crop parts of the image to vary the object's scale and position within the frame.\n",
        "\n",
        "Noise Addition: Add random noise (e.g., Gaussian noise) to the image to simulate noise in real-world data.\n",
        "\n",
        "Color Jitter: Randomly change the color saturation, hue, and brightness to introduce color variations.\n",
        "\n",
        "Blur: Apply random blur to the image to simulate motion or out-of-focus conditions.\n",
        "\n",
        "Here's a high-level example of how to implement image augmentations using Python and OpenCV:\n",
        "\n",
        "import cv2 import numpy as np"
      ],
      "metadata": {
        "id": "XqjXUbBqNxCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load an image"
      ],
      "metadata": {
        "id": "raQWQTIEN7yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "image = cv2.imread('image.jpg')"
      ],
      "metadata": {
        "id": "f-oT2QQdOB25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define augmentation parameters"
      ],
      "metadata": {
        "id": "LzCd-dJDOChW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "angle_range = 15 # Rotation angle range in degrees brightness_range = 0.5 # Brightness adjustment range contrast_range = 0.5 # Contrast adjustment range"
      ],
      "metadata": {
        "id": "zDMTDEkVOHRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomly rotate the image"
      ],
      "metadata": {
        "id": "zN-mHS5YONae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "angle = np.random.uniform(-angle_range, angle_range) rows, cols, _ = image.shape rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1) image = cv2.warpAffine(image, rotation_matrix, (cols, rows))"
      ],
      "metadata": {
        "id": "BPfaxcDHOSRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomly adjust brightness and contrast"
      ],
      "metadata": {
        "id": "27zCS_6BOWK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "brightness = 1.0 + np.random.uniform(-brightness_range, brightness_range) contrast = 1.0 + np.random.uniform(-contrast_range, contrast_range) image = cv2.convertScaleAbs(image, alpha=brightness, beta=contrast)"
      ],
      "metadata": {
        "id": "bYA4f9feOd6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display the augmented image"
      ],
      "metadata": {
        "id": "ib03r-KUOhw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cv2.imshow('Augmented Image', image) cv2.waitKey(0) cv2.destroyAllWindows() You can create a pipeline of such augmentations and apply them to your training data before feeding it into your deep learning model. Experiment with different augmentation settings and combinations to see how they impact your model's performance in terms of accuracy and generalization.\n",
        "\n",
        "4-ANSWER: I'm unable to directly perform image augmentations, train models, or conduct experiments to determine the highest accuracy achievable or the smallest training dataset size required to achieve an accuracy above 50% for a specific task. The actual results will depend on a variety of factors, including the complexity of the task, the architecture of the neural network, the quality of the data, and the chosen augmentations.\n",
        "\n",
        "To achieve the highest accuracy and determine the smallest training dataset size that can yield an accuracy above 50%, you'll need to follow a systematic experimental process. Here are the general steps to conduct such experiments:\n",
        "\n",
        "Define your task: Clearly define the image classification task you want to solve. Identify the dataset you'll use for training and testing.\n",
        "\n",
        "Data preprocessing: Preprocess your dataset, including resizing, normalization, and splitting it into training and validation sets.\n",
        "\n",
        "Augmentation pipeline: Create an augmentation pipeline that includes a combination of augmentations such as rotation, flip, brightness adjustment, zoom, and others, as discussed earlier.\n",
        "\n",
        "Model architecture: Choose or design an appropriate neural network architecture for your task. This can be a convolutional neural network (CNN) for image classification.\n",
        "\n",
        "Training setup: Set up your training process, including batch size, learning rate, and number of epochs. Use the augmented training data to train the model.\n",
        "\n",
        "Evaluate and fine-tune: After training, evaluate your model's accuracy on a separate validation set. If the accuracy is below 50%, consider adjusting your augmentation pipeline, model architecture, or training parameters.\n",
        "\n",
        "Iterate: Repeat the training, evaluation, and fine-tuning process, adjusting hyperparameters and augmentations as needed until you achieve the desired accuracy.\n",
        "\n",
        "The highest achievable accuracy and the minimum training dataset size for a specific task can vary widely and will depend on the complexity of the task itself. Tasks with simpler patterns may require smaller datasets, while more complex tasks may require larger datasets and more sophisticated models.\n",
        "\n",
        "It's also important to remember that achieving high accuracy may not be the only goal. Depending on the task, you may need to consider other metrics such as precision, recall, F1-score, or even domain-specific metrics.\n",
        "\n",
        "In practice, experimentation and iterative development are key to finding the right combination of data, model, and augmentation techniques that yield the best results for your specific image classification task."
      ],
      "metadata": {
        "id": "4lJnkAOYOlqr"
      }
    }
  ]
}